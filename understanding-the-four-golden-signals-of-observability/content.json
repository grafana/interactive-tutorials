{
  "id": "understanding-the-four-golden-signals-of-observability",
  "title": "Understanding the Four Golden Signals of Observability",
  "blocks": [
    {
      "type": "section",
      "blocks": [
        {
          "type": "markdown",
          "content": "This guide will explain the **Four Golden Signals** of Observability **- Latency, Traffic, Errors and Saturation -** with conceptual explanations and examples of how they can be visualized in a dashboard.\n\n# Introduction\n\nIn modern software systems observability is essential to maintaining reliability and performance. One of the most influential frameworks for measuring system health and user experience is the **Four Golden Signals**, popularized by Google’s Site Reliability Engineering (SRE) practices and first published in the [Google SRE Book in 2017](https://sre.google/sre-book/).\n\nBefore that, system monitoring was often ad hoc, focusing on host-level metrics (CPU, memory, disk). As systems grew into **complex, distributed architectures**, Google engineers realized traditional monitoring wasn’t enough — they needed a **universal, user-centric framework**.\n\nThe Four Golden Signals evolved as a set of metrics to describe the health of *any* service from an end-user perspective.\n\nOver time, this model became foundational to modern Observability practices.\n\n"
        },
        {
          "type": "markdown",
          "content": "# The Four Golden Signals\n\nThe Signals are explained in the [SRE Book](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals) as following:\n\n## Latency\n\n### Definition\n\nThe time it takes to service a request. It’s important to distinguish between the latency of successful requests and the latency of failed requests. For example, an HTTP 500 error triggered due to loss of connection to a database or other critical backend might be served very quickly; however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result in misleading calculations. On the other hand, a slow error is even worse than a fast error! Therefore, it’s important to track error latency, as opposed to just filtering out errors.\n\n### Why it matters\n\nHigh latency directly affects user experience. Monitoring latency helps identify performance bottlenecks or overloaded components.\n\n## Traffic\n\nA measure of how much demand is being placed on your system, measured in a high-level system-specific metric. For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might be transactions and retrievals per second.\n\n### Why it matters\n\nTracking traffic helps understand normal load patterns, forecast scaling needs, and detect sudden spikes (potential DDoS attacks or runaway clients).\n\n## Errors\n\nThe rate of requests that fail, either explicitly (e.g., HTTP 500s), implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy (for example, \"If you committed to one-second response times, any request over one second is an error\"). Where protocol response codes are insufficient to express all failure conditions, secondary (internal) protocols may be necessary to track partial failure modes. Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent job of catching all completely failed requests, while only end-to-end system tests can detect that you’re serving the wrong content.\n\n### Why it matters\n\nAn increase in error rates can indicate system bugs, dependency issues, or deployment regressions.\n\n## Saturation\n\nHow \"full\" your service is. A measure of your system fraction, emphasizing the resources that are most constrained (e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O). Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target is essential.\nIn complex systems, saturation can be supplemented with higher-level load measurement: can your service properly handle double the traffic, handle only 10% more traffic, or handle even less traffic than it currently receives? For very simple services that have no parameters that alter the complexity of the request (e.g., \"Give me a nonce\" or \"I need a globally unique monotonic integer\") that rarely change configuration, a static value from a load test might be adequate. As discussed in the previous paragraph, however, most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound. Latency increases are often a leading indicator of saturation. Measuring your 99th percentile response time over some small window (e.g., one minute) can give a very early signal of saturation.\nFinally, saturation is also concerned with predictions of impending saturation, such as \"It looks like your database will fill its hard drive in 4 hours.\"\n\n## Why it matters\n\nA saturated system cannot process additional load efficiently. Monitoring saturation prevents cascading failures by identifying resource limits.\n\n"
        }
      ],
      "id": "guide-section-77662",
      "title": "Introduction"
    },
    {
      "type": "interactive",
      "action": "navigate",
      "reftarget": "/dashboards/f/grafana-demodashboards-quickpizza",
      "content": "# Example Dashboards\n\nLet's look at some dashboards that use those Golden Signals. Let's open the Grafana Quick Pizza Demo Dashboards"
    },
    {
      "type": "guided",
      "content": "Open the Performance Stats Dashboard",
      "steps": [
        {
          "action": "highlight",
          "reftarget": "a[href='/d/a581fb5a-df38-45d7-83cb-d10835930fa1/performance-stats']"
        }
      ]
    },
    {
      "type": "guided",
      "content": "The first panel shows the first Golden Signal Latency",
      "steps": [
        {
          "action": "hover",
          "reftarget": "section[data-testid='data-testid Panel header Frontend Response Latency'] div:nth-match(7)"
        }
      ]
    }
  ]
}