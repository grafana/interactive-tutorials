{
  "id": "infrastructure-alerting-planning-alerts",
  "title": "Plan your alert rules",
  "blocks": [
    {
      "type": "markdown",
      "content": "Before you create an alert rule, take a moment to plan. Good alerts are specific, actionable, and appropriately urgent. Poor alerts create noise that leads to alert fatigueâ€”and eventually, ignored pages."
    },
    {
      "type": "markdown",
      "content": "Effective alert planning answers three questions: What should I monitor? When should it fire? Who should be notified?"
    },
    {
      "type": "markdown",
      "content": "To plan your alert rules, consider the following:"
    },
    {
      "type": "markdown",
      "content": "**1. Choose what to monitor.** Start with metrics or logs that directly indicate user impact or system health.\n\n| Data type | What to monitor | Why it matters |\n|-----------|-----------------|----------------|\n| **Metrics** | CPU, memory, disk, network | Resource exhaustion affects all services |\n| **Logs** | Error patterns, exceptions, failed requests | Application health and user impact |"
    },
    {
      "type": "markdown",
      "content": "**2. Define meaningful thresholds.** Base thresholds on what \"normal\" looks like in your environment, not arbitrary numbers.\n\n| Data type | Example threshold | Reasoning |\n|-----------|-------------------|-----------||\n| **Metrics** | CPU > 80% | Normal is 40-60%, gives time to respond |\n| **Logs** | Errors > 10/min | Normal is 1-2/min, catches real spikes |"
    },
    {
      "type": "markdown",
      "content": "**3. Set appropriate urgency.** Not every alert needs to page someone at 3 AM.\n\n| Alert type | Metrics example | Logs example | Urgency |\n|------------|-----------------|--------------|---------||\n| **Critical** | Disk 95% full | `FATAL` or `panic` logs | Page immediately |\n| **Warning** | CPU elevated 15 min | Error rate 5x normal | Slack notification |\n| **Info** | Memory trending up | Unusual log pattern | Email digest |"
    },
    {
      "type": "markdown",
      "content": "**4. Identify the responders.** Who should receive this alert? The platform team? Database team? On-call engineer?"
    },
    {
      "type": "markdown",
      "content": "**5. Consider the \"for\" duration.** How long should the condition persist before firing? Brief spikes during deployments shouldn't page anyone."
    },
    {
      "type": "markdown",
      "content": "In the next milestone, you'll use Grafana's exploration tools to find the specific metrics or logs you want to alert on."
    },
    {
      "type": "markdown",
      "content": "**More to explore (optional)**\n\n- [Alerting best practices](/docs/grafana/latest/alerting/best-practices/)"
    }
  ]
}
